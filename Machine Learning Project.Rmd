---
title: "Machine Learning Project"
output: 
    html_document: 
        keep_md: true
---

## Introduction
The purpose of this exercise is to apply machine learning algorithms to a prediction problem in Human Activity Recognition (HAR). Our data comes from the Weight Lifting Exercises (WLE) Dataset from the paper Qualitative Activity Recognition of Weight Lifting Exercises by Velloso, Bulling, Gellersen, Ugulino & Fuks. The aim is to predict the manner in which an exercise (Unilateral Dumbbell Bicep Curl) was being carried out by looking at data collected from a set of wearable sensors (Qualitative Activity Recognition). The sensors (mounted to 4 spots: belt, arm, forearm & dumbbell) records accelaration, gyroscope and magnetometer readings in 3 dimension. 

It should be noted for this exercise unlike the original paper, we did not split the data into windows and use summary statistics as predictors. Instead, our models were trained on the raw readings therefore might be subjected to higher degree of overfitting. 

We applied machine learning algorithms on 52 continuous predictors to predict which class (A,B,C,D or E) our response falls into. Classification Tree (CART) and Linear Discriminant Analysis (LDA) were first used. However, they did not performed well on the test set: CART's accuracy is 49% while LDA's is 70%. We then proceed on to ensemble models. We applied Random Forest and found it highly accurate in predicting the response class thus selected it as our final model. 10-fold cross validation was used to estimate the out-of-sample error rate (~0.5%). The chosen model was applied to twenty test cases and is able to predict the correct class for all twenty cases.
  
## Analysis
### Preparing required packages
```{r, warning=FALSE, message=FALSE}
library(caret)
library(randomForest)
set.seed(628)
```

### Loading training data and test cases
```{r,cache=TRUE}
rawTraining <- read.csv("pml-training.csv")
rawTestCases <- read.csv("pml-testing.csv")
unused_col <- which(is.na(rawTestCases[1,]))
names(rawTraining)[-unused_col]
```
The first 7 columns does not contain useful information for prediction. Therefore they are excluded.

### Data splitting
```{r,cache=TRUE}
unused_col <- c(1:7,unused_col)
inTrain <- createDataPartition(rawTraining$classe,p=0.7,list=FALSE)
training <- rawTraining[inTrain,-unused_col]
testing <- rawTraining[-inTrain,-unused_col]
```
We splited the training data into 70% training set and 30% test sets.

### Model selection and training
```{r,cache=TRUE, warning=FALSE, message=FALSE}
model1 <- train(classe~.,data=training,method="rpart")
pred1 <- predict(model1,testing)
confusionMatrix(pred1,testing$classe)
model2 <- train(classe~.,data=training,method="lda")
pred2 <- predict(model2,testing)
confusionMatrix(pred2,testing$classe)
```
As can be seen, accuracy for the CART and LDA models are relatively low. For all the classes, sensitivity hardly exceeds 80%. Same for positive predictive value.

### Using ensemble models
We see if ensemble models can give us better result. Random Forest was used:
```{r,cache=TRUE, warning=FALSE, message=FALSE}
model3 <- train(classe~.,data=training,method="rf",trControl=trainControl(method="oob"))
pred3 <- predict(model3,testing)
confusionMatrix(pred3,testing$classe)
```
Results from Random Forest were optimistic with accuracy close to 99% and sensitivity and positive predictive value exceeding 99% for all classes. Random Forest is chosen as our final model.
  
## Estimating Out-of-Sample error rate with cross validation
```{r,cache=TRUE}
model3$bestTune
folds <- createFolds(rawTraining$classe,k=10)
compare_perf <- matrix(,nrow=0,ncol=7)
topTenImpVar <- matrix(,nrow=10,ncol=0)
for(k in 1:10){
    modelk <- randomForest(classe~.,data=rawTraining[-folds[[k]],-unused_col],mtry=2)
    predk <- predict(modelk,rawTraining[folds[[k]],-unused_col])
    cfM <- confusionMatrix(predk,rawTraining[folds[[k]],"classe"])
    compare_perf <- rbind(compare_perf,cfM$overall)
    vI <- varImp(modelk)
    topTenImpVar <- cbind(topTenImpVar,rownames(vI)[order(vI,decreasing=TRUE)][1:10])
}
compare_perf[,1:4]
colMeans(compare_perf)[1:4]
```
Using 10-fold cross validation, we estimate out-of-sample error rate to be around 0.5%.
  
## Applying the final model in prediction
```{r,cache=TRUE}
finalModel <- randomForest(classe~.,rawTraining[,-unused_col],mtry=2)
```
```{r,fig.height=7}
varImpPlot(finalModel, main="Variable Importance Plot")
```

The variable importance plot shows the most important variables for identifying the response's class is (1) Roll of belt sensor, (2) Yaw of belt sensor and (3) Magnetometer reading in the z direction. We compare this to the trained predictors from our 10-fold cross validation.

```{r}
topTenImpVar
```
We can see the top 10 ranking are fairly consistent and also agree with the list for the final model which was applied on the entire training data. This shows the final chosen model is reliable across different sets of training data thus give us further confidence in our result.
  
We applied our chosen model to the test cases and obtained the following results:
```{r}
answer <- predict(finalModel,rawTestCases); answer
```
  
## Reference
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
  
# End